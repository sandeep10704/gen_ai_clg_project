{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d634b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57cc186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandeep/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sandeep/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/hf-inference/models/facebook/bart-large-cnn\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1cc0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary(text, ratio=1.2):\n",
    "    words = word_tokenize(text)\n",
    "    filtered = [w.lower() for w in words if w.isalnum() and w.lower() not in STOP_WORDS]\n",
    "\n",
    "    freq_table = {}\n",
    "    for w in filtered:\n",
    "        freq_table[w] = freq_table.get(w, 0) + 1\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freq_table.items():\n",
    "            if word in sentence.lower():\n",
    "                sentence_scores[sentence] = sentence_scores.get(sentence, 0) + freq\n",
    "\n",
    "    if not sentence_scores:\n",
    "        return text\n",
    "\n",
    "    avg_score = sum(sentence_scores.values()) / len(sentence_scores)\n",
    "    summary = \" \".join([s for s in sentences if sentence_scores.get(s, 0) > ratio * avg_score])\n",
    "    return summary if summary.strip() else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd19c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=400):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield \" \".join(words[i:i+max_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52cecbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractive_summary(text, min_length=40, max_length=150):\n",
    "    payload = {\"inputs\": text, \"parameters\": {\"min_length\": min_length, \"max_length\": max_length, \"do_sample\": False}}\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=HEADERS, json=payload, timeout=60)\n",
    "        return response.json()[0][\"summary_text\"]\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b669a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_summarize(text, extract_ratio=1.2, chunk_size=500, min_len=120, max_len=200):\n",
    "    extract = extractive_summary(text, ratio=extract_ratio)\n",
    "    final_summaries = []\n",
    "    for chunk in chunk_text(extract, max_words=chunk_size):\n",
    "        summary = abstractive_summary(chunk, min_length=min_len, max_length=max_len)\n",
    "        final_summaries.append(summary)\n",
    "# Optionally, summarize the combined summaries\n",
    "    combined_summary = \" \".join(final_summaries)\n",
    "    if len(final_summaries) > 1:\n",
    "        combined_summary = abstractive_summary(combined_summary, min_length=min_len, max_length=max_len)\n",
    "    return combined_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dd9a843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing represents one of the most profound technological advances of the 21st century. Unlike classical computers, which operate on bits in deterministic states of 0 or 1, quantum computers utilize qubits that exploit principles such as superposition and entanglement. Quantum computing has the potential to transform multiple fields including cryptography, artificial intelligence, materials science, and complex optimization problems. For more information, visit quantum-computing.org and the University of Washington ’s Quantum Computing Center.’ for more information on the University of Washington’ ‘Quantum Computing Center’, click here.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"\"\"\n",
    "Quantum computing represents one of the most profound technological advances of the 21st century, with the potential to transform multiple fields including cryptography, artificial intelligence, materials science, and complex optimization problems. Unlike classical computers, which operate on bits in deterministic states of 0 or 1, quantum computers utilize qubits that exploit principles such as superposition and entanglement, enabling the simultaneous exploration of multiple computational pathways. This capability allows quantum machines to solve certain classes of problems exponentially faster than classical counterparts, potentially addressing challenges that have remained intractable for decades. Despite the promise, the development of scalable, fault-tolerant quantum computers faces formidable engineering hurdles. Qubits are highly susceptible to decoherence caused by environmental interference, and error correction schemes often require the use of thousands of physical qubits to construct a single logical qubit. Different physical implementations—ranging from superconducting circuits to trapped ions and photonic systems—offer varying trade-offs in terms of scalability, stability, and operational complexity. Current quantum processors operate at cryogenic temperatures, necessitating sophisticated cooling infrastructures and extremely precise control mechanisms. Beyond hardware, the creation of efficient quantum algorithms is crucial, as naive translation of classical algorithms often fails to harness the full potential of quantum architectures. Fields such as cryptography are particularly affected, with quantum computing threatening to undermine classical public-key cryptosystems like RSA and ECC, prompting research into post-quantum cryptography. In drug discovery and molecular simulation, quantum computers can model complex chemical interactions at atomic resolutions, potentially accelerating the development of new therapeutics and materials. However, practical applications are currently constrained by limited qubit counts and coherence times, requiring hybrid approaches that combine classical and quantum computing. Despite these limitations, experimental demonstrations of quantum supremacy have shown that quantum devices can outperform classical computers on specific, well-defined tasks. As research continues, the integration of quantum computing into mainstream technology could redefine computational problem-solving, influence global security paradigms, and catalyze breakthroughs across scientific disciplines. The trajectory of this field is highly dynamic, demanding interdisciplinary expertise in physics, computer science, engineering, and mathematics to address both theoretical and practical challenges. The ultimate success of quantum computing will depend not only on technological advancements but also on ethical, economic, and societal considerations surrounding accessibility, control, and the responsible deployment of transformative computational capabilities.\n",
    "\"\"\"  \n",
    "summary = hybrid_summarize(input_text, extract_ratio=1.2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693882f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
